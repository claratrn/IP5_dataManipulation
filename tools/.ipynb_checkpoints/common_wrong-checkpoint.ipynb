{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745007c2b7a95956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T18:54:11.243100Z",
     "start_time": "2024-06-05T18:54:10.915951Z"
    }
   },
   "source": [
    "# Jupyter Notebook Script\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from CSV files\n",
    "data_llama2 = pd.read_csv('../results/llama2/gsm8k/zero_shot_cot_gsm8k/20240603-112531205664_289c2160bbb5425186b314e0fb9e4551_answered.csv')\n",
    "data_openai = pd.read_csv('../results/openAi/gsm8k/zero_shot_cot_gsm8k/20240525-174150582952_72dcbbf5bc124790b148e5ddce48fc59_answered.csv')\n",
    "\n",
    "# Display the first few rows of each dataframe to understand their structure\n",
    "print(\"Llama2 Data:\")\n",
    "display(data_llama2.head())\n",
    "\n",
    "print(\"OpenAI Data:\")\n",
    "display(data_openai.head())\n",
    "\n",
    "# Calculate the number of questions answered correctly by each model\n",
    "num_correct_llama2 = data_llama2['correct'].sum()\n",
    "num_correct_openai = data_openai['correct'].sum()\n",
    "\n",
    "# Total number of questions in the dataset\n",
    "total_questions_llama2 = len(data_llama2)\n",
    "total_questions_openai = len(data_openai)\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Total number of answered questions in Llama2 dataset: {total_questions_llama2}\")\n",
    "print(f\"Number of questions answered correctly by Llama-2: {num_correct_llama2}\")\n",
    "print(f\"Total number of answered questions in OpenAI dataset: {total_questions_openai}\")\n",
    "print(f\"Number of questions answered correctly by OpenAI GPT-3.5 Turbo: {num_correct_openai}\")\n",
    "\n",
    "# Find common mistakes where both models made errors\n",
    "common_mistakes = data_llama2.merge(data_openai, on='question', suffixes=('_llama', '_openai'))\n",
    "common_mistakes = common_mistakes[(common_mistakes['correct_llama'] == False) & (common_mistakes['correct_openai'] == False)]\n",
    "\n",
    "directory = '../result_responses'\n",
    "summary_file_path = os.path.join(directory, 'correct_answers_summary.txt')\n",
    "common_mistakes_file_path = os.path.join(directory, 'common_mistakes.csv')\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Display the common mistakes\n",
    "print(\"Common mistakes where both models failed:\")\n",
    "display(common_mistakes[['question', 'predicted label_llama', 'predicted label_openai', 'ground truth_llama', 'ground truth_openai']])\n",
    "\n",
    "# Save the common mistakes to a CSV file\n",
    "common_mistakes.to_csv('/mnt/data/common_mistakes.csv', index=False)\n",
    "\n",
    "# Save the summary of correct answers to a text file\n",
    "with open(summary_file_path, 'w') as f:\n",
    "    f.write(f\"Total number of answered questions in Llama2 dataset: {total_questions_llama2}\\n\")\n",
    "    f.write(f\"Number of questions answered correctly by Llama-2: {num_correct_llama2}\\n\")\n",
    "    f.write(f\"Total number of answered questions in OpenAI dataset: {total_questions_openai}\\n\")\n",
    "    f.write(f\"Number of questions answered correctly by OpenAI GPT-3.5 Turbo: {num_correct_openai}\\n\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92bc481921b6c",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
